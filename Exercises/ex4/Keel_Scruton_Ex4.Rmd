---
title: "Ex4_Keel_Scruton"
output: Github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(arrow)
library(ggraph)
library(igraph)
library(wru)
library(gender)
library(stringr)
library(mice)
```
Note: all the first section here is a direct copy of ex3 work, some explanations have been left out as such. 

## Load data

Load the following data:
  + applications from `app_data_sample.parquet`
  + edges from `edges_sample.csv`

```{r load-data}
data_path <- "/Users/keelscruton/Desktop/Org Network Analysis/672_project_data/"
applications <- read_parquet(paste0(data_path,"app_data_sample.parquet"))
edges <- read_csv(paste0(data_path,"edges_sample.csv"))
applications
edges

```

Determine the gender for each examiner 
```{r}
library(gender)
#install_genderdata_package() # only run this line the first time you use the package, to get data for it
# get a list of first names without repetitions
examiner_names <- applications %>% 
  distinct(examiner_name_first)
examiner_names


# get a table of names and gender
examiner_names_gender <- examiner_names %>% 
  do(results = gender(.$examiner_name_first, method = "ssa")) %>% 
  unnest(cols = c(results), keep_empty = TRUE) %>% 
  select(
    examiner_name_first = name,
    gender,
    proportion_female
  )
examiner_names_gender

```
final step in determining gender by name...
```{r}
# remove extra colums from the gender table
examiner_names_gender <- examiner_names_gender %>% 
  select(examiner_name_first, gender)
# joining gender back to the dataset
applications <- applications %>% 
  left_join(examiner_names_gender, by = "examiner_name_first")
# cleaning up
rm(examiner_names)
rm(examiner_names_gender)
gc()
```


## Guess the examiner's race

We'll now use package `wru` to estimate likely race of an examiner. Just like with gender, we'll get a list of unique names first, only now we are using surnames.

```{r race-1}
library(wru)
examiner_surnames <- applications %>% 
  select(surname = examiner_name_last) %>% 
  distinct()
examiner_surnames
```

```{r race-2}
examiner_race <- predict_race(voter.file = examiner_surnames, surname.only = T) %>% 
  as_tibble()
examiner_race
```

```{r race-3}
examiner_race <- examiner_race %>% 
  mutate(max_race_p = pmax(pred.asi, pred.bla, pred.his, pred.oth, pred.whi)) %>% 
  mutate(race = case_when(
    max_race_p == pred.asi ~ "Asian",
    max_race_p == pred.bla ~ "black",
    max_race_p == pred.his ~ "Hispanic",
    max_race_p == pred.oth ~ "other",
    max_race_p == pred.whi ~ "white",
    TRUE ~ NA_character_
  ))
examiner_race
```

```{r race-4}
# removing extra columns
examiner_race <- examiner_race %>% 
  select(surname,race)
applications <- applications %>% 
  left_join(examiner_race, by = c("examiner_name_last" = "surname"))
rm(examiner_race)
rm(examiner_surnames)
gc()
```

Determine tenure as well as a number of days...
```{r tenure-1}
library(lubridate) # to work with dates
examiner_dates <- applications %>% 
  select(examiner_id, filing_date, appl_status_date) 
examiner_dates
```

```{r tenure-2}
examiner_dates <- examiner_dates %>% 
  mutate(start_date = ymd(filing_date), end_date = as_date(dmy_hms(appl_status_date)))
```

```{r tenure-3}
examiner_dates <- examiner_dates %>% 
  group_by(examiner_id) %>% 
  summarise(
    earliest_date = min(start_date, na.rm = TRUE), 
    latest_date = max(end_date, na.rm = TRUE),
    tenure_days = interval(earliest_date, latest_date) %/% days(1)
    ) %>% 
  filter(year(latest_date)<2018)
examiner_dates
```

```{r tenure-4}
applications <- applications %>% 
  left_join(examiner_dates, by = "examiner_id")
rm(examiner_dates)
gc()
```

Application Processing Time section (new for ex4)

```{r}
applications$appl_end_date <- paste(applications$patent_issue_date, applications$abandon_date, sep=',')
# clean  the column by removing instances of commas and NA's
applications$appl_end_date <- gsub('NA', "", as.character(applications$appl_end_date))
applications$appl_end_date <- gsub(',', "", as.character(applications$appl_end_date))
# make date format  consistent
applications$appl_end_date <- as.Date(applications$appl_end_date, format="%Y-%m-%d")
applications$filing_date <- as.Date(applications$filing_date, format="%Y-%m-%d")
# calculate the difference in days between the application end date and the filing date
applications$appl_proc_days <- as.numeric(difftime(applications$appl_end_date, applications$filing_date, units=c("days")))
# Remove data points where the filing date is after the issue/abandon dates, this is not possible
applications <- applications %>% filter(appl_proc_days >=0 | appl_proc_days != NA)
```
want only unique instances 
```{r}
vars <- c("gender","race","tenure_days","appl_proc_days")
applications = drop_na(applications,any_of(vars))

```


make my group selection 
```{r}
group_161 <- applications[substr(applications$examiner_art_unit, 1, 3) == 161,]
group_161 <-group_161[row.names(unique(group_161[,"examiner_id"])),]


group_162 <- applications[substr(applications$examiner_art_unit, 1, 3) == 162,]
group_162 <-group_162[row.names(unique(group_162[,"examiner_id"])),]
```

Create advice networks from `edges_sample` and calculate centrality scores for examiners in your selected workgroups
```{r}
#create distinct subset of examiners with only the art unit and examiner id to be able to re join onto our other data afterwards
examiner_dis = distinct(subset(applications, select = -c(filing_date, abandon_date, earliest_date, appl_end_date, appl_status_date, patent_issue_date, latest_date, examiner_name_middle, patent_number)))
examiner_dis$group = substr(examiner_dis$examiner_art_unit, 1,3)
#get rid of all examiners except those in group 161 or 162
examiner_dis = examiner_dis[examiner_dis$group==161 | examiner_dis$group==162,]
```


Now that we have a list of the examiners who are part of work groups 161 and 162 we can combine (merge) it with the edge list (edges) this will allow us to form our subset network. 

```{r}
#edges_examiner = merge(x=edges, y=examiner_dis, by.x="ego_examiner_id", by.y="examiner_id", all.x=TRUE)
#edges_examiner = edges_examiner %>% rename(ego_au=examiner_art_unit, ego_group=group)

#edges_examiner = merge(x=edges_examiner, y=examiner_dis, by.x="alter_examiner_id", by.y="examiner_id", all.x=TRUE)
#edges_examiner = edges_examiner %>% rename(alter_au=examiner_art_unit, alter_group=group)

#edges_examiner = drop_na(edges_examiner) #drop all na, ie values not in the selected workgroups. 





##^^^ ERROR HAPPENING HERE I HAVE COMMENTED IT OUT SO THAT I CAN KNIT IT. 
```

Now the above data set has the edges of alter and ego examiners. we can next create the list of nodes for both the ego and alter examiners. 

```{r}
#Ego_nodes = subset(edges_examiner, select=c(ego_examiner_id,ego_au, ego_group)) %>% rename(examiner_id=ego_examiner_id,art_unit=ego_au,group=ego_group)

#Alter_nodes = subset(edges_examiner, select=c(alter_examiner_id,alter_au, alter_group))%>% rename(examiner_id=alter_examiner_id,art_unit=alter_au,group=alter_group)

#Nodes_full = distinct(rbind(Ego_nodes, Alter_nodes))

#Nodes_full is a list of all the distinct examiners involved in the advice network

```

Now we can create the graph network. 
```{r}
#ran into error where i had duplicated vertex names, to fix take the first instances only in the nodes list. 

#Nodes_full = Nodes_full %>% group_by(examiner_id) %>% summarise(examiner_id=first(examiner_id), art_unit=first(art_unit), group=first(group))

#network <- graph_from_data_frame(d=edges_examiner, vertices=Nodes_full, directed=TRUE)
```

```{r}
#Degree <- degree(network)
#Closeness <- closeness(network)
#merge back into the dataframe...
#comp <- data.frame(Nodes_full, Degree, Closeness)  
#applications_final <- merge(x=applications, y=comp, by='examiner_id', all.x=TRUE)
```

Note: ran into an error above: Vector memory exhausted, not sure how to approach this differently to avoid having such a big data set being merged.  note that the lm() work would look something like the following but the code does not run. 

lm <- lm(appl_proc_days~ Degree + Closeness+ gender + tenure_days, data=applications_final)
summary(lm1)

by observing the estimated coefficients from this step we could make some observations on what variables are most affecting application processing time (days)

we could then next create an interaction variable to discover the interaction between some demographic data and centrality (degree, closeness) this would look like the following. note that we could swap our gender for race. 


lm <- lm(appl_proc_days~ Degree + Closeness+ gender + tenure_days +Degree*gender + Closeness*gender, data=applications_final)
summary(lm1)


the findings from this could help us discover if there is some subgroup that is better at processing applications faster. 




